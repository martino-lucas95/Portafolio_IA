{% extends 'portfolio/base.html' %}
{% load static %}

{% block title %}Reducción de Dimensionalidad{% endblock %}

{% block content %}
<div class="container">
    <h1 class="text-center mb-4">Subtipos de Reducción de Dimensionalidad</h1>
    
    <div class="row">
        <div class="col-md-6">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">PCA (Principal Component Analysis)</h5>
                    <p class="card-text">
                        PCA es una técnica que transforma los datos en nuevas variables (componentes principales), que capturan la mayor varianza posible en menos dimensiones.
                    </p>
                    <h6>¿Cuándo usarlo?</h6>
                    <p>Es útil cuando los datos tienen muchas características correlacionadas. PCA es común en visualización de datos y preprocesamiento para modelos supervisados y no supervisados.</p>
                    <h6>¿Qué necesita?</h6>
                    <p>Es necesario que los datos estén escalados (normalizados o estandarizados), ya que PCA es sensible a las magnitudes de las características.</p>
                </div>
            </div>
        </div>

        <div class="col-md-6">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">LDA (Linear Discriminant Analysis)</h5>
                    <p class="card-text">
                        LDA busca maximizar la separación entre clases mientras reduce las dimensiones. Es un método supervisado que proyecta los datos en una nueva base que separa mejor las clases.
                    </p>
                    <h6>¿Cuándo usarlo?</h6>
                    <p>LDA es ideal cuando se tiene un problema de clasificación y se necesita reducir las dimensiones mientras se mantiene la capacidad de separar las clases.</p>
                    <h6>¿Qué necesita?</h6>
                    <p>Necesita que los datos estén etiquetados (supervisado) y que estén escalados. Es útil cuando hay múltiples clases en los datos.</p>
                </div>
            </div>
        </div>
    </div>

    <div class="row mt-4">
        <div class="col-md-6">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">Forward Selection</h5>
                    <p class="card-text">
                        Forward Selection es un método de selección de características que comienza con un modelo vacío y agrega iterativamente las características que mejoran el rendimiento del modelo.
                    </p>
                    <h6>¿Cuándo usarlo?</h6>
                    <p>Es útil cuando se desea encontrar un subconjunto óptimo de características para mejorar el rendimiento del modelo, especialmente en datasets con muchas variables.</p>
                    <h6>¿Qué necesita?</h6>
                    <p>Requiere un criterio para evaluar el modelo en cada paso (como el error cuadrático medio o el puntaje F1) y puede ser computacionalmente costoso si el número de características es muy alto.</p>
                </div>
            </div>
        </div>

        <div class="col-md-6">
            <div class="card">
                <div class="card-body">
                    <h5 class="card-title">Backward Selection</h5>
                    <p class="card-text">
                        Backward Selection es un método de selección de características que comienza con todas las características y elimina iterativamente aquellas que menos contribuyen al modelo.
                    </p>
                    <h6>¿Cuándo usarlo?</h6>
                    <p>Es útil cuando se tiene un dataset con muchas características y se busca simplificar el modelo sin comprometer significativamente su precisión.</p>
                    <h6>¿Qué necesita?</h6>
                    <p>Requiere un criterio de evaluación y puede ser más adecuado para datasets con menos características debido a su complejidad computacional.</p>
                </div>
            </div>
        </div>
    </div>

    <div class="description mt-5">
        <h3>¿Cuándo aplicar Reducción de Dimensionalidad?</h3>
        <p>
            Se aplica cuando los datos tienen demasiadas características y se quiere reducir la complejidad sin perder mucha información. Es útil en preprocesamiento para mejorar el rendimiento de los modelos de Machine Learning.
        </p>
        <h4>¿Qué precondiciones se deben cumplir?</h4>
        <ul>
            <li><strong>Escalado de datos:</strong> Es crucial para técnicas como PCA y LDA, que son sensibles a las magnitudes de las características.</li>
            <li><strong>Relaciones entre características:</strong> Técnicas como PCA funcionan mejor cuando las características están correlacionadas. Si no lo están, es posible que no se obtenga una gran reducción de dimensionalidad.</li>
            <li><strong>Evaluación de modelos:</strong> Métodos como Forward y Backward Selection requieren una métrica clara para evaluar el rendimiento del modelo en cada iteración.</li>
        </ul>
    </div>
</div>
{% endblock %}
